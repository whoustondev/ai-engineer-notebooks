{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06e5882a-52f9-47a0-8a9c-183b3353b99e",
      "metadata": {
        "id": "06e5882a-52f9-47a0-8a9c-183b3353b99e"
      },
      "source": [
        "# Agenda\n",
        "- Installation and import of neccessary packages\n",
        "- Download neccesary corpus from NLTK\n",
        "- Data Cleaning\n",
        "  - Tokenization\n",
        "  - Changing case\n",
        "  - Spelling correction\n",
        "  - POS Tagging\n",
        "  - Named Entity Recognition\n",
        "  - Stemming and  Lemmetization\n",
        "  - Noise Entity Removal\n",
        "    - Remove stop words\n",
        "    - remove urls\n",
        "    - remove punctuation\n",
        "    - remove emoticons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "bcfaedb9-d500-48fe-a1fc-125ccf48edda",
      "metadata": {
        "id": "bcfaedb9-d500-48fe-a1fc-125ccf48edda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a19fd49a-0175-45b4-cfd6-e297a430e85f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Requirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy==1.13.1) (1.26.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: svgling in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.11/dist-packages (from svgling) (1.4.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: pyldavis in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from pyldavis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyldavis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyldavis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyldavis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyldavis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyldavis) (2.10.2)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.11/dist-packages (from pyldavis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyldavis) (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (from pyldavis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyldavis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyldavis) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyldavis) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->pyldavis) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyldavis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->pyldavis) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install --upgrade nltk\n",
        "!pip install numpy==1.26.4\n",
        "!pip install pandas==2.2.2\n",
        "!pip install scipy==1.13.1\n",
        "!pip install gensim\n",
        "!pip install svgling\n",
        "!pip install spacy\n",
        "!pip install pyldavis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3cc1a2f3-a62e-4ba0-ad86-5ae9ac10e55b",
      "metadata": {
        "id": "3cc1a2f3-a62e-4ba0-ad86-5ae9ac10e55b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86555996-f9ac-4e0c-c854-8c825f5c6ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "cdcdf433-be09-4e30-bb1f-1d4275337cdd",
      "metadata": {
        "id": "cdcdf433-be09-4e30-bb1f-1d4275337cdd"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "from string import punctuation\n",
        "import nltk\n",
        "# import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "1b877aaf-cf0c-4c28-943d-07ef780e36e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b877aaf-cf0c-4c28-943d-07ef780e36e2",
        "outputId": "e3319279-4829-40c2-f74c-5eaafccd0447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "nltk.download('popular')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b4bfe6fd-43ca-450f-92d2-8e1356d2cfd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4bfe6fd-43ca-450f-92d2-8e1356d2cfd4",
        "outputId": "8f944591-714d-45ac-d4da-6b2fb06717bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "131d38f4-7eb7-440c-be43-f16eb36151df",
      "metadata": {
        "id": "131d38f4-7eb7-440c-be43-f16eb36151df"
      },
      "outputs": [],
      "source": [
        "# import necessary corpuses\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "1ec598b5-9b9f-4174-9b69-4fd0a51bcdc5",
      "metadata": {
        "id": "1ec598b5-9b9f-4174-9b69-4fd0a51bcdc5"
      },
      "outputs": [],
      "source": [
        "## Data Cleaning steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "71f1daf5-86db-4a0c-8033-11e14d78b203",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71f1daf5-86db-4a0c-8033-11e14d78b203",
        "outputId": "4a35bdd1-d876-4a1a-f1cc-7b45c791fd1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "       sentiment                                             review\n",
            "0              1             Good but need updates and improvements\n",
            "1              0  Worst mobile i have bought ever, Battery is dr...\n",
            "2              1  when I will get my 10% cash back.... its alrea...\n",
            "3              1                                               Good\n",
            "4              0  The worst phone everThey have changed the last...\n",
            "...          ...                                                ...\n",
            "14670          1  I really like the phone, Everything is working...\n",
            "14671          1  The Lenovo K8 Note is awesome. It takes best p...\n",
            "14672          1                       Awesome Gaget.. @ this price\n",
            "14673          1  This phone is nice processing will be successf...\n",
            "14674          1      Good product but the pakeging was not enough.\n",
            "\n",
            "[14675 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def read_csv(file_path):\n",
        "    # Check if the file is a zip file\n",
        "    if file_path.endswith('.zip'):\n",
        "        # Handle reading a CSV file inside the ZIP archive\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            # List the contents of the ZIP to get the CSV filename inside\n",
        "            zip_contents = zip_ref.namelist()\n",
        "            # Assuming there's only one CSV file inside the zip\n",
        "            csv_file_name = zip_contents[0]\n",
        "            with zip_ref.open(csv_file_name) as file:\n",
        "                # Read the CSV file directly into a pandas DataFrame\n",
        "                df = pd.read_csv(file)\n",
        "    else:\n",
        "        # If it's not a zip, read the CSV directly\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage:\n",
        "file_path = '/content/drive/MyDrive/AI Engineer Simplilearn Course/NLP/k8s-csv.csv.zip'  # Replace with the actual file path (zip or csv)\n",
        "df = read_csv(file_path)\n",
        "\n",
        "# Output the dataframe\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Extract the text from the second column (assuming the second column is named 'Column2')\n",
        "# Replace 'Column2' with the actual name of the column or use index if necessary (e.g., df.iloc[:, 1])\n",
        "reviews_list = df.iloc[:, 1].str.lower().tolist()\n",
        "\n",
        "# Output the list of texts\n",
        "\n"
      ],
      "metadata": {
        "id": "O7ouFB224_5Q"
      },
      "id": "O7ouFB224_5Q",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7930e885-2875-430b-9759-f1889e8b8010",
      "metadata": {
        "id": "7930e885-2875-430b-9759-f1889e8b8010"
      },
      "outputs": [],
      "source": [
        "## Tokenization\n",
        "### Sentence Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "tokenized_reviews = [word_tokenize(review) for review in reviews_list]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "4959c8f8-a2a7-4278-80b2-781170e1aca4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4959c8f8-a2a7-4278-80b2-781170e1aca4",
        "outputId": "3bdf7725-7802-43d3-87de-4d7188e4d0a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'but', 'need', 'updates', 'and', 'improvements']\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_reviews[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "8f6cfb69-761e-4d99-a166-784ef1e9001f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f6cfb69-761e-4d99-a166-784ef1e9001f",
        "outputId": "a770163a-b762-4f2e-e08f-4f3f3039e3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "nltk.download('punkt')  # For tokenization\n",
        "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
        "nltk.download('stopwords')  # (optional) For stopwords if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6e342209-685e-4755-bad9-a54596cf70aa",
      "metadata": {
        "id": "6e342209-685e-4755-bad9-a54596cf70aa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "de2c5c6f-053b-4680-b9e7-26b26243bb64",
      "metadata": {
        "id": "de2c5c6f-053b-4680-b9e7-26b26243bb64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5867e67-e8a5-4bb1-e268-abbcebbddcc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## get the list ofenglish words\n",
        "\n",
        "words =  nltk.corpus.words.words()\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "tagged_reviews = [pos_tag(review) for review in tokenized_reviews]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "229fa929-efd8-4481-badd-862aee58ba63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "229fa929-efd8-4481-badd-862aee58ba63",
        "outputId": "fdd07ca9-432a-4917-be66-f639534f2625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words in the  vocab 236736\n"
          ]
        }
      ],
      "source": [
        "print(f'Total number of words in the  vocab {len(words)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "a7446b47-5578-40f4-b5c0-97fe6571b1f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7446b47-5578-40f4-b5c0-97fe6571b1f9",
        "outputId": "6ae6d004-4e80-4357-be31-bf85072f5edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('good', 'JJ'), ('but', 'CC'), ('need', 'VBP'), ('updates', 'NNS'), ('and', 'CC'), ('improvements', 'NNS')]\n"
          ]
        }
      ],
      "source": [
        "print(tagged_reviews[0])\n",
        "\n",
        "# Step 5: Filter for only nouns (NN, NNS, NNP, NNPS)\n",
        "nouns_list = []\n",
        "\n",
        "for tagged_review in tagged_reviews:\n",
        "    nouns_in_review = [word for word, tag in tagged_review if tag.startswith('NN')]  # Keep only nouns\n",
        "    nouns_list.append(nouns_in_review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "6b050917-ceee-4c51-aaae-2c362093a514",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b050917-ceee-4c51-aaae-2c362093a514",
        "outputId": "e840b889-dc32-4223-8f05-f4a38e2abffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['updates', 'improvements']\n"
          ]
        }
      ],
      "source": [
        "print(nouns_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Step 4: Define POS tags that correspond to nouns\n",
        "# Noun POS tags: 'NN' (singular), 'NNS' (plural), 'NNP' (proper singular), 'NNPS' (proper plural)\n",
        "noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "\n",
        "# Step 5: Lemmatize and remove stopwords and punctuation\n",
        "# A lemmatizer is a tool in natural language processing (NLP) that reduces a word to its base or root form, known as the lemma. Unlike stemming, which simply chops off word endings to create a root word, lemmatization ensures that the word is reduced to a meaningful base form according to its correct part of speech (POS).\n",
        "# For example:\n",
        "# \"running\" becomes \"run\"\n",
        "# \"better\" becomes \"good\"\n",
        "# \"geese\" becomes \"goose\"\n",
        "\n",
        "# stop word = a common word (like \"the,\" \"a,\" \"is,\" \"and\") that is often filtered during text analysis/search indexing\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(['.', ',', '!', '?', ';', ':', '(', ')', '-', '_', '\"', \"'\"])\n",
        "\n",
        "\n",
        "# Step 3: Clean the reviews by lemmatizing, removing stopwords, and punctuation\n",
        "cleaned_reviews = []\n",
        "for tagged_review in tagged_reviews:\n",
        "    cleaned_review = [\n",
        "        lemmatizer.lemmatize(word.lower())  # Lemmatize and convert to lowercase\n",
        "        for word, tag in tagged_review\n",
        "        if tag in noun_tags and word.lower() not in stop_words and word not in punctuation\n",
        "    ]\n",
        "    cleaned_reviews.append(cleaned_review)\n",
        "\n",
        "# Step 4: Create a dictionary and corpus for topic modeling\n",
        "dictionary = corpora.Dictionary(cleaned_reviews)\n",
        "corpus = [dictionary.doc2bow(review) for review in cleaned_reviews]\n",
        "\n",
        "# Step 5: Build the LDA model with 12 topics\n",
        "lda_model = LdaModel(corpus, num_topics=12, id2word=dictionary, passes=15)\n",
        "\n",
        "# Step 6: Display the top terms for each topic\n",
        "for idx, topic in lda_model.print_topics(num_words=20):\n",
        "    print(f\"Topic {idx}: {topic}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQHVRSXEr40g",
        "outputId": "98b4cdad-9737-4970-9823-c6187538030f"
      },
      "id": "qQHVRSXEr40g",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 0.208*\"issue\" + 0.114*\"network\" + 0.060*\"month\" + 0.031*\"buy\" + 0.029*\"everything\" + 0.025*\"lot\" + 0.023*\"box\" + 0.023*\"......\" + 0.022*\"thanks\" + 0.020*\"connectivity\" + 0.019*\"plz\" + 0.014*\"r\" + 0.013*\"worth\" + 0.012*\"headset\" + 0.012*\"offer\" + 0.011*\"help\" + 0.010*\"dont\" + 0.010*\"buying\" + 0.010*\"....\" + 0.008*\"update\"\n",
            "\n",
            "\n",
            "Topic 1: 0.143*\"battery\" + 0.111*\"phone\" + 0.093*\"problem\" + 0.048*\"day\" + 0.035*\"heating\" + 0.034*\"issue\" + 0.027*\"time\" + 0.027*\"hour\" + 0.024*\"backup\" + 0.021*\"%\" + 0.017*\"use\" + 0.016*\"life\" + 0.015*\"month\" + 0.014*\"charge\" + 0.013*\"drain\" + 0.011*\"service\" + 0.011*\"usage\" + 0.010*\"hr\" + 0.010*\"lenovo\" + 0.010*\"update\"\n",
            "\n",
            "\n",
            "Topic 2: 0.118*\"charger\" + 0.080*\"feature\" + 0.054*\"turbo\" + 0.044*\"android\" + 0.041*\"stock\" + 0.025*\"piece\" + 0.021*\"mi\" + 0.020*\"charging\" + 0.020*\"item\" + 0.017*\"cell\" + 0.016*\"work\" + 0.015*\"cable\" + 0.014*\"dolby\" + 0.013*\"storage\" + 0.012*\"charge\" + 0.011*\"box\" + 0.010*\"ringtone\" + 0.010*\"space\" + 0.010*\"pro\" + 0.010*\"amount\"\n",
            "\n",
            "\n",
            "Topic 3: 0.238*\"camera\" + 0.106*\"quality\" + 0.068*\"battery\" + 0.059*\"performance\" + 0.049*\"phone\" + 0.026*\"speaker\" + 0.024*\"....\" + 0.018*\"processor\" + 0.018*\"backup\" + 0.017*\"speed\" + 0.016*\"h\" + 0.015*\"mark\" + 0.014*\"clarity\" + 0.012*\"ram\" + 0.011*\"image\" + 0.011*\"photo\" + 0.011*\"look\" + 0.011*\"everything\" + 0.010*\"core\" + 0.009*\"flash\"\n",
            "\n",
            "\n",
            "Topic 4: 0.330*\"product\" + 0.071*\"device\" + 0.022*\"earphone\" + 0.018*\"app\" + 0.016*\"lenovo\" + 0.016*\"application\" + 0.015*\"volume\" + 0.014*\"gallery\" + 0.013*\"apps\" + 0.011*\"....\" + 0.011*\"auto\" + 0.010*\"time\" + 0.009*\"player\" + 0.009*\"music\" + 0.008*\"party\" + 0.008*\"setting\" + 0.008*\"o\" + 0.008*\"sale\" + 0.007*\"google\" + 0.007*\"fine\"\n",
            "\n",
            "\n",
            "Topic 5: 0.235*\"mobile\" + 0.162*\"note\" + 0.090*\"k8\" + 0.061*\"lenovo\" + 0.019*\"system\" + 0.017*\"dolby\" + 0.016*\"headphone\" + 0.014*\"atmos\" + 0.013*\"version\" + 0.013*\"sound\" + 0.011*\"k4\" + 0.011*\"awesome\" + 0.008*\"pls\" + 0.007*\"k5\" + 0.005*\"invoice\" + 0.005*\"seller\" + 0.005*\"online\" + 0.005*\"fone\" + 0.005*\"superb\" + 0.004*\"way\"\n",
            "\n",
            "\n",
            "Topic 6: 0.263*\"..\" + 0.040*\"hai\" + 0.022*\"expectation\" + 0.021*\"picture\" + 0.018*\"gb\" + 0.016*\"ho\" + 0.016*\"performance\" + 0.012*\"fast\" + 0.011*\"average\" + 0.011*\"hi\" + 0.010*\"bill\" + 0.010*\"gud\" + 0.009*\"sound\" + 0.009*\"bhi\" + 0.008*\"ram\" + 0.007*\"ye\" + 0.006*\"ka\" + 0.006*\"ki\" + 0.006*\"se\" + 0.006*\"raha\"\n",
            "\n",
            "\n",
            "Topic 7: 0.088*\"money\" + 0.079*\"amazon\" + 0.072*\"product\" + 0.045*\"service\" + 0.041*\"waste\" + 0.033*\"value\" + 0.031*\"delivery\" + 0.030*\"customer\" + 0.027*\"return\" + 0.025*\"experience\" + 0.021*\"replacement\" + 0.017*\"time\" + 0.015*\"lenovo\" + 0.015*\"care\" + 0.014*\"support\" + 0.013*\"model\" + 0.011*\"refund\" + 0.009*\"response\" + 0.008*\"complaint\" + 0.008*\"need\"\n",
            "\n",
            "\n",
            "Topic 8: 0.085*\"camera\" + 0.050*\"display\" + 0.045*\"mode\" + 0.028*\"quality\" + 0.027*\"glass\" + 0.026*\"sound\" + 0.025*\"depth\" + 0.022*\"screen\" + 0.019*\"front\" + 0.018*\"music\" + 0.016*\"nothing\" + 0.015*\"time\" + 0.014*\"sensor\" + 0.014*\"gorilla\" + 0.014*\"mp\" + 0.013*\"hand\" + 0.013*\"cost\" + 0.012*\"day\" + 0.011*\"key\" + 0.011*\"thing\"\n",
            "\n",
            "\n",
            "Topic 9: 0.065*\"call\" + 0.052*\"screen\" + 0.047*\"option\" + 0.032*\"software\" + 0.028*\"feature\" + 0.027*\"update\" + 0.020*\"cast\" + 0.017*\"lenovo\" + 0.015*\"function\" + 0.015*\"notification\" + 0.015*\"app\" + 0.014*\"contact\" + 0.013*\"voice\" + 0.013*\"data\" + 0.012*\"power\" + 0.012*\"time\" + 0.012*\"button\" + 0.011*\"bug\" + 0.011*\"message\" + 0.011*\"video\"\n",
            "\n",
            "\n",
            "Topic 10: 0.411*\"phone\" + 0.081*\"price\" + 0.032*\"range\" + 0.029*\"feature\" + 0.022*\"sim\" + 0.020*\".....\" + 0.017*\"budget\" + 0.013*\"jio\" + 0.013*\"*\" + 0.012*\"time\" + 0.012*\"superb\" + 0.011*\"volta\" + 0.010*\"processor\" + 0.010*\"....\" + 0.009*\"memory\" + 0.009*\"card\" + 0.009*\"slot\" + 0.008*\"lot\" + 0.008*\"game\" + 0.007*\"review\"\n",
            "\n",
            "\n",
            "Topic 11: 0.112*\"heat\" + 0.070*\"handset\" + 0.044*\"smartphone\" + 0.036*\"bit\" + 0.034*\"killer\" + 0.032*\"star\" + 0.029*\"set\" + 0.029*\"ok\" + 0.024*\"cam\" + 0.019*\"purchase\" + 0.017*\"date\" + 0.017*\"please\" + 0.015*\"improvement\" + 0.014*\"get\" + 0.013*\"brand\" + 0.013*\"deal\" + 0.012*\"hang\" + 0.012*\"processing\" + 0.011*\"complain\" + 0.011*\"process\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Assuming you have the LDA model and corpus already created, like this:\n",
        "# lda_model = gensim.models.LdaModel(corpus, num_topics=10, id2word=dictionary)\n",
        "\n",
        "# Compute the coherence score using the 'c_v' metric\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=cleaned_reviews, dictionary=dictionary, coherence='c_v')\n",
        "\n",
        "# Get the coherence score\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "\n",
        "print(f'Coherence Score (c_v): {coherence_lda}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_2mbYGBoRu7",
        "outputId": "3f7abc86-acb6-4200-81c0-44e341bd3fa1"
      },
      "id": "R_2mbYGBoRu7",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score (c_v): 0.531863028078864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming you've already processed your corpus and created a dictionary\n",
        "\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Recreate the LDA model with the optimal number of topics (6)\n",
        "lda_model_optimal = LdaModel(corpus, num_topics=6, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print out the top 10 terms for each topic\n",
        "topics = lda_model_optimal.print_topics(num_words=20)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZaT8DeitN0J",
        "outputId": "5fb8ec0b-5802-44a4-e475-83a1fdf14bc2"
      },
      "id": "uZaT8DeitN0J",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.083*\"note\" + 0.056*\"k8\" + 0.028*\"lenovo\" + 0.021*\"hai\" + 0.019*\"speaker\" + 0.016*\"h\" + 0.012*\"headphone\" + 0.012*\"dolby\" + 0.011*\"sound\" + 0.010*\"system\" + 0.009*\"atmos\" + 0.009*\"ho\" + 0.008*\"condition\" + 0.007*\"k4\" + 0.007*\"earphone\" + 0.005*\"volume\" + 0.005*\"version\" + 0.005*\"hi\" + 0.005*\"k\" + 0.005*\"player\"')\n",
            "(1, '0.199*\"product\" + 0.055*\"money\" + 0.050*\"amazon\" + 0.033*\"phone\" + 0.028*\"service\" + 0.026*\"....\" + 0.026*\"waste\" + 0.022*\"delivery\" + 0.021*\"value\" + 0.017*\"return\" + 0.016*\"customer\" + 0.014*\"price\" + 0.014*\"lenovo\" + 0.011*\"time\" + 0.011*\"experience\" + 0.009*\"policy\" + 0.009*\"thanks\" + 0.009*\"......\" + 0.008*\"day\" + 0.007*\"pls\"')\n",
            "(2, '0.166*\"phone\" + 0.045*\"camera\" + 0.021*\"price\" + 0.017*\"feature\" + 0.016*\"device\" + 0.016*\"quality\" + 0.013*\"lenovo\" + 0.012*\"issue\" + 0.011*\"battery\" + 0.011*\"processor\" + 0.011*\"update\" + 0.010*\"software\" + 0.010*\"performance\" + 0.009*\"time\" + 0.009*\"range\" + 0.008*\"mode\" + 0.008*\"day\" + 0.007*\"game\" + 0.007*\"thing\" + 0.007*\"budget\"')\n",
            "(3, '0.130*\"camera\" + 0.101*\"mobile\" + 0.061*\"quality\" + 0.042*\"battery\" + 0.039*\"..\" + 0.022*\"heat\" + 0.016*\"performance\" + 0.015*\"....\" + 0.013*\"feature\" + 0.012*\"backup\" + 0.011*\"everything\" + 0.011*\"price\" + 0.011*\".....\" + 0.010*\"clarity\" + 0.009*\"mark\" + 0.008*\"mode\" + 0.008*\"picture\" + 0.008*\"sound\" + 0.007*\"super\" + 0.007*\"front\"')\n",
            "(4, '0.038*\"screen\" + 0.037*\"call\" + 0.035*\"charger\" + 0.031*\"phone\" + 0.029*\"problem\" + 0.016*\"handset\" + 0.015*\"option\" + 0.014*\"glass\" + 0.014*\"month\" + 0.014*\"superb\" + 0.013*\"service\" + 0.011*\"cast\" + 0.011*\"day\" + 0.011*\"*\" + 0.011*\"note\" + 0.011*\"center\" + 0.011*\"display\" + 0.009*\"model\" + 0.009*\"time\" + 0.009*\"replacement\"')\n",
            "(5, '0.110*\"battery\" + 0.080*\"phone\" + 0.059*\"problem\" + 0.056*\"issue\" + 0.046*\"..\" + 0.029*\"heating\" + 0.029*\"network\" + 0.024*\"hour\" + 0.023*\"day\" + 0.021*\"time\" + 0.019*\"performance\" + 0.019*\"backup\" + 0.018*\"%\" + 0.016*\"charge\" + 0.014*\"sim\" + 0.011*\"drain\" + 0.011*\"use\" + 0.010*\"month\" + 0.010*\"hr\" + 0.010*\"life\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the coherence score using the 'c_v' metric\n",
        "coherence_model_optimal = CoherenceModel(model=lda_model_optimal, texts=cleaned_reviews, dictionary=dictionary, coherence='c_v')\n",
        "\n",
        "# Get the coherence score\n",
        "coherence_optimal = coherence_model_optimal.get_coherence()\n",
        "\n",
        "print(f'Coherence Score (c_v) for Optimal Model: {coherence_optimal}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdSLrZcJranN",
        "outputId": "163ceb62-463e-4464-cd11-fb7e72e4d6aa"
      },
      "id": "bdSLrZcJranN",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score (c_v) for Optimal Model: 0.5939567690036128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pyLDAvis.gensim_models\n",
        "# pyLDAvis.enable_notebook()\n",
        "# vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "# pyLDAvis.display(vis)"
      ],
      "metadata": {
        "id": "CL7xyZj7t0cG"
      },
      "id": "CL7xyZj7t0cG",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Raw topic data\n",
        "topics = [\n",
        "    (0, '0.203*\"mobile\" + 0.081*\"money\" + 0.038*\"price\" + 0.038*\"waste\" + 0.034*\"feature\" + 0.030*\"value\" + 0.028*\"range\" + 0.025*\"superb\" + 0.010*\"class\" + 0.009*\"awesome\" + 0.009*\"everything\" + 0.006*\"rest\" + 0.005*\"item\" + 0.005*\"headset\" + 0.004*\"good\" + 0.004*\"cam\" + 0.004*\"half\" + 0.004*\"buying\" + 0.004*\"set\" + 0.004*\"budget\"'),\n",
        "    (1, '0.145*\"battery\" + 0.089*\"phone\" + 0.051*\"problem\" + 0.043*\"issue\" + 0.028*\"backup\" + 0.027*\"heating\" + 0.027*\"day\" + 0.024*\"camera\" + 0.024*\"time\" + 0.022*\"hour\" + 0.016*\"%\" + 0.016*\"life\" + 0.016*\"heat\" + 0.015*\"performance\" + 0.012*\"charge\" + 0.012*\"month\" + 0.011*\"use\" + 0.010*\"drain\" + 0.010*\"charging\" + 0.010*\"network\"'),\n",
        "    (2, '0.178*\"product\" + 0.119*\"..\" + 0.051*\"....\" + 0.027*\"amazon\" + 0.023*\"performance\" + 0.021*\".....\" + 0.020*\"delivery\" + 0.019*\"price\" + 0.013*\"service\" + 0.011*\"*\" + 0.011*\"lenovo\" + 0.011*\"experience\" + 0.011*\"processor\" + 0.011*\"smartphone\" + 0.009*\"thanks\" + 0.008*\"device\" + 0.008*\"super\" + 0.008*\"......\" + 0.007*\"time\" + 0.007*\"ok\"'),\n",
        "    (3, '0.062*\"phone\" + 0.047*\"note\" + 0.030*\"lenovo\" + 0.024*\"k8\" + 0.023*\"screen\" + 0.020*\"call\" + 0.018*\"problem\" + 0.018*\"issue\" + 0.014*\"service\" + 0.013*\"time\" + 0.013*\"option\" + 0.012*\"device\" + 0.011*\"feature\" + 0.011*\"update\" + 0.011*\"speaker\" + 0.010*\"day\" + 0.009*\"software\" + 0.008*\"network\" + 0.007*\"system\" + 0.007*\"support\"'),\n",
        "    (4, '0.135*\"camera\" + 0.068*\"quality\" + 0.058*\"phone\" + 0.015*\"mode\" + 0.015*\"performance\" + 0.014*\"feature\" + 0.014*\"sim\" + 0.013*\"sound\" + 0.012*\"price\" + 0.010*\"ram\" + 0.009*\"display\" + 0.009*\"photo\" + 0.009*\"depth\" + 0.008*\"picture\" + 0.008*\"video\" + 0.008*\"music\" + 0.008*\"network\" + 0.008*\"speed\" + 0.008*\"clarity\" + 0.007*\"sensor\"'),\n",
        "    (5, '0.205*\"phone\" + 0.022*\"charger\" + 0.019*\"hai\" + 0.015*\"return\" + 0.015*\"h\" + 0.011*\"price\" + 0.009*\"worth\" + 0.008*\"policy\" + 0.008*\"ho\" + 0.007*\"box\" + 0.007*\"budget\" + 0.007*\"amazon\" + 0.007*\"month\" + 0.006*\"refund\" + 0.006*\"purchase\" + 0.005*\"earphone\" + 0.005*\"bill\" + 0.005*\"cable\" + 0.005*\"glass\" + 0.005*\"cell\"')\n",
        "]\n",
        "\n",
        "# Labels for the topics (manual interpretation of top words)\n",
        "topic_labels = [\n",
        "    \"Product Value and Features\",      # Topic 0\n",
        "    \"Battery and Heating Issues\",      # Topic 1\n",
        "    \"Product Delivery and Service\",    # Topic 2\n",
        "    \"Phone Model and Updates\",         # Topic 3\n",
        "    \"Camera and Phone Quality\",        # Topic 4\n",
        "    \"Phone Purchase and Returns\"       # Topic 5\n",
        "]\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "topics_data = []\n",
        "\n",
        "# Loop over the topics and extract top words for each\n",
        "for topic_id, terms in topics:\n",
        "    # Extract top 10 terms (splitting the terms from the model output string)\n",
        "    top_terms = [term.split('*')[1].strip('\"') for term in terms.split(' + ')][:10]\n",
        "    topics_data.append([topic_labels[topic_id], \", \".join(top_terms)])\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "df_topics = pd.DataFrame(topics_data, columns=[\"Topic Name\", \"Top 10 Terms\"])\n",
        "\n",
        "# Display the table\n",
        "print(df_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVgzqi-KQwPx",
        "outputId": "0c96ec6a-5574-4983-d702-b7e7a1408cdb"
      },
      "id": "tVgzqi-KQwPx",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Topic Name  \\\n",
            "0    Product Value and Features   \n",
            "1    Battery and Heating Issues   \n",
            "2  Product Delivery and Service   \n",
            "3       Phone Model and Updates   \n",
            "4      Camera and Phone Quality   \n",
            "5    Phone Purchase and Returns   \n",
            "\n",
            "                                        Top 10 Terms  \n",
            "0  mobile, money, price, waste, feature, value, r...  \n",
            "1  battery, phone, problem, issue, backup, heatin...  \n",
            "2  product, .., ...., amazon, performance, .....,...  \n",
            "3  phone, note, lenovo, k8, screen, call, problem...  \n",
            "4  camera, quality, phone, mode, performance, fea...  \n",
            "5  phone, charger, hai, return, h, price, worth, ...  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}